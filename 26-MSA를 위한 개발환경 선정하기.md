## Preamble


쿠버네티스에 배포할 MSA를 개발할 때, 개발환경을 구성하는 다양한 방법들이 존재한다. 이 글에서는 


1. Docker Compose (이하 도커 컴포즈), 로컬 쿠버네티스, 클라우드 개발서버 구축 방식의 장단점
2. MSA 프로젝트를 공유저장소에 모노/멀티 레포지토리로 관리하는 것의 장단점 

에 대해 알아보고, 나의 프로젝트 및 팀의 특성에 기반해 개발환경으로 어떤 방식을 채택했는지 과정을 기술해보려한다.

## Docker Compose


도커 컴포즈는 여러 개의 컨테이너를 묶어서 작업할 수 있게 해주는 도구이다. 서비스 디스커버리, 전체 컨테이너 이미지 일괄 빌드 및 컨테이너 셍성, 컨테이너간 하나의 통일된 네트워크 구성을 가능하게 해준다.

### 모노레포 장점


도커 컴포즈 + 모노레포일 때, 가장 큰 장점은 개발환경에서부터 모든 서비스를 동시에 확인하면서 개발할 수 있어서 서비스 간 통신을 확인하기 편하고 가시성이 보장된다는 것이다.

하나의 도커 컴포즈 네트워크로 묶인 모든 서비스는 하나의 도커 컴포즈 명령어로 한 번에 실행될 수 있고, FE/BE간, 또 BE/BE간 구현이 의도한대로 잘 이루어졌는지 확인하며 개발하기 쉽다.

### 모노레포 단점

- 깃옵스 배포의 한계

깃허브의 웹훅을 이용한 깃옵스 배포를 완벽히 구현하기 어려워진다. 깃허브는 레포지토리 전체적으로 푸시가 일어났을 떄 웹훅을 보낸다. 레포지토리의 특정 디렉토리만의 변경을 감지하진 못한다. 

만약 모노레포로 MSA 프로젝트가 구성된다면, 서비스 A를 업데이트해서 배포하기 위해 아무런 변경이 되지 않은 서비스 B, C도 함께 빌드되고 배포되어야한다. 

이것은 MSA의 주요 원칙은 마이크로 서비스 별 독립적 배포 및 운영이 불가능해짐을 의미한다.

따라서 깃옵스를 이용한 파이프라인을 구축하는 대신, 업데이트가 완료되고 마스터브랜치에 푸시가 성공적으로 이루어진 후에 미리 설정해둔 마이크로서비스 별 젠킨스 Job을 실행시켜야한다. 

그냥 젠킨스에 접속해서 Job 버튼 하나만 누르면 해결되는 일이긴 하지만, 푸시만 해도 알아서 배포까지 해주는 깃옵스 파이프라인을 구성하지 못한다는 것은 잦은 업데이트가 필요한 프로젝트에는 큰 단점으로 작용할 수 있다.


- 버전 관리의 어려움

레포지토리가 하나로 묶여있기 때문에, 모든 서비스들의 커밋도 한 곳에 모이게 된다. 프로젝트 내부적으로 커밋 컨벤션을 정의해서 어떤 서비스의 커밋인지 알아보기 쉽게 할 수는 있겠지만 , 어찌됐든 관리의 복잡성이 증가하는 것은 피할 수 없게 된다.

### 멀티레포 장점


모노레포와는 다르게 각 서비스 별 레포지토리가 분리되어있기 때문에, 배포 이후 운영환경에서 깃허브 웹훅과 젠킨스 파이프라인을 통해 서비스 별 개별적인 배포를 깃옵스 방식으로 자동화시킬 수 있다. 

또한 레포지토리가 서비스 별로 분리되어있기 때문에 서비스 별 가독성이 좋고, 커밋 역시 분리되어있기 떄문에 서비스 별 버전관리 등이 모노레포에 비해 아주 효과적이다.

### 멀티레포 단점


레포지토리를 분리해도 도커 컴포즈를 사용할 수는 있다. 


1. 도커파일과 도커 컴포즈 파일을 관리하는 별도의 레포지토리를 구성하고, 각 도커파일은 빌드될 때마다 담당 서비스의 소스코드를 공유저장소에서 Pull 해와서 컨테이너를 생성하는 방식
2. 각 서비스마다 도커컴포즈를 사용하는 방식
- 1번 단점 : 볼륨 설정 불가

1번 방식의 단점으로는 수정한 코드에 대한 결과를 확인하기까지의 복잡성이 크다는 것이다. 도커 컴포즈와 도커파일이 모두 소스코드 레포지토리와 별도로 존재하기 때문에 소스 코드가 실행되는 도커 컨테이너 내부와 소스 코드를 작성하는 로컬을 볼륨으로 이어줄 수가 없다.

볼륨 설정이 불가능하다는 것은 핫/라이브 리로딩을 사용하지 못한다는 것을 의미하고, 변경된 소스코드의 결과를 확인하고 싶을 때마다 서비스 레포지토리에 커밋을 하고, PR을 보내고, 리뷰를 받고, 마스터에 머지를 하고, 도커 컴포즈와 도커파일이 위치한 레포지토리로 이동해서, 도커 컴포즈 명령을 실행해야 확인할 수 있게된다.

그럼 도커파일에서 서비스 레포지토리를 git clone으로 Pull 해와서 변경된 소스코드를 가지고 빌드하게 될 것이다. 이 작업은 단순히 함수명 한 글자를 대문자로 바꾸기만해도 항상 시행해야하는 단계들이다. 이런 복잡성이 개발기간을 불필요하게 증가시킬 수 있다.


- 2번 단점 : 다른 서비스들과의 테스트 불가

레포지토리가 분리되어있기 때문에, 각 레포지토리에서 실행된 도커 컴포즈들은 각각 독립적인 네트워크를 가지게된다. 그 네트워크들이 차라리 클라우드에 있어서 모두 개별적인 PublicIP가 존재한다면 CORS 설정을 통해 서로 통신할 수 있겠지만, 로컬에서 가상화되어 서로 다른 네트워크로 분리된 서비스들은 서로간의 통신이 불가능해지게 된다.

프론트엔드마저 분리되어있기 떄문에, 백엔드 개발자는 모든 서비스들의 개발이 끝나고 QA로 스테이징하기 전까지는 결과물을 눈으로 확인할 수 없게된다.

이렇게 다른 서비스들과의 통신을 확인하지 못한채 개발하기 때문에, 이후 QA를 진행할 때, 실제 Staging 서버에 서비스들을 배포하고 테스트를 하게 되면, 다른 방식보다 훨씬 더 많은 문제들이 나오게 될 것이고, QA 과정에서 해야할 일들과 비용이 늘어나게 된다.

## Local Kubernetes


KIND, minikube 등의 로컬 쿠버네티스 구축 도구를 사용해 개발할 수 있다. 우선 개발환경으로 로컬 쿠버네티스를 효과적으로 사용하기 위해서는 모노레포가 전제되어야한다. 멀티레포의 프로젝트를 로컬 쿠버네티스 위에 배포하기 위해서는 하루의 개발을 시작할 때마다 모든 레포지토리를 Pull 해와야한다. 서비스의 수(멀티레포에서의 레포지토리의 수)가 적다면 다행이지만, 100개, 1000개가 넘어가면 공유저장소에서 코드를 Pull 해오다가 하루가 다 지나갈 것이다.

### 장점


위에 언급한 레포지토리를 합칠 때의 장점들에 추가적으로 운영환경으로 이전(QA/배포)할 때 비용(시간, 노력)이 적게든다. 개발환경을 구성하면서 서비스에 필요한 쿠버네티스 manifest들이 정의되어있고, 이 manifest들은 운영환경에 어플리케이션을 배포할 때도 재사용 가능한 코드들이기 때문에 상대적으로 편리하다.

또 쿠버네티스에서의 서비스간 통신 뿐만 아니라, Istio 등 쿠버네티스 위에 올라가는 여러가지 툴들을 개발하면서 설정해둘 수 있기 때문에 운영환경으로의 배포가 쉬워진다.

### 단점

- 엄청난 리소스 사용량

위에 언급한 레포지토리를 합칠 때의 단점들에 추가적으로 KIND, Istio 등의 툴들을 로컬에 설치해야하기 때문에, 일정 수준 이상의 로컬 성능이 요구되고,  실제 개발하는 동안에 리소스가 많이 사용되게 된다. 일례로, Istio의  minimum requirements는 4GB의 RAM이고, Kubernetes는 2GB의 RAM, Kafka는 8GB의 RAM을 필요로 한다. 아무 코드도 작성하지 않아도 벌써 14GB의 메모리 공간을 할당해두게 되는 것이다.

## Cloud Dev-Server


가장 깔끔하고, 경험해보진 않았지만 실무에서 가장 많이 사용하는 방식일 것 같다. 

실제 어플리케이션이 운영될 클라우드 서버에 (배포를 온프레미스로 한다면 온프레미스 서버에) 개발환경을 구축해서 개발/QA를 모두 진행하고, 그대로 배포까지 이어가는 것이다.

만약 클라우드 개발서버를 구축해서 개발을 하면 레포지토리는 분리해야한다. 멀티레포로 프로젝트를 구성했을 때의 단점이 없기 때문에 멀티레포로 구성해서 깃옵스 방식을 적용한 파이프라인의 장점을 최대한으로 이용할 수 있다.

### 장점


앞서 살펴본 모든 방식들은 도커 컴포즈를 사용하든, 레포지토리를 분리하든 합치든, 로컬 쿠버네티스를 사용하든, 어쨌든 각 방식의 단점이 존재한다.

이 단점들은 클라우드 개발서버를 구축하면 해결될 수 있다. 엄청난 장점이지만, 그만큼 엄청난 단점이 있다. 

### 단점


바로 비용이다.  나의 경우 어림잡아 개발 기간을 3개월, QA 기간 1달, 이후 운영기간 최소 1달을 잡고 계산해보니 전체 클라우드 서버 비용이 약 70~90만원에 달할 것으로 계산이 되었다.

## Summary


정리를 하자면 아래와 같다.

### 금전적 비용이 적게 드는 순서


로컬 쿠버네티스 >>> 도커컴포즈 모노레포 >>> 도커컴포즈 멀티레포 >>> 클라우드 개발서버


- 도커컴포즈 모노레포는 쿠버네티스 리소스 구성부터 Istio 등의 다른 쿠버네티스 위에서 돌아가는 툴들을 처음부터 구성해야하기 때문에 QA를 진행하는 과정이 로컬 쿠버네티스보다 상대적으로 오래 걸린다.
- 도커컴포즈 멀티레포는 개발환경에서 다른 서비스들간의 연결 확인을 하지 못했기 때문에 모노레포 방식보다 네트워크적으로 더 구성해야할 것이 많다.
- 클라우드 개발서버는 개발단계부터 서버 비용을 지불한다.

### 개발(FE/BE/DB) 편의성이 높은 순서


도커컴포즈 모노레포 >>> 클라우드 개발서버 = 로컬 쿠버네티스 >>> 도커컴포즈 멀티레포


- 도커 컴포즈 모노레포는 핫/라이브 리로딩을 활용한 개발이 가능하다.
- 클라우드 개발서버와 로컬 쿠버네티스는 변경사항을 적용한 도커 이미지를 빌드하고 푸시하는 과정을 거쳐야한다.
- 도커컴포즈 멀티레포는 개발환경에서 다른 서비스들과의 연결이 불가능하다.

### 배포 자동화(Devops) 효율이 높은 순서


클라우드 개발서버 = 도커컴포즈 멀티레포 >  도커컴포즈 모노레포 = 로컬 쿠버네티스


- 클라우드 개발서버와 도커컴포즈 멀티레포는 깃옵스 파이프라인 구축이 가능하다.
- 로컬 쿠버네티스와 도커컴포즈 모노레포는 깃옵스 파이프라인 구축이 효과적이지 않다.

### 개발환경 구축 난이도가 높은 순서


클라우드 개발서버 = 로컬 쿠버네티스 >>> 도커컴포즈 모노레포 >>> 도커컴포즈 멀티레포


- 클라우드 개발서버와 로컬 쿠버네티스는 초기 구성부터 운영환경을 거의 다 구현해둔다.
- 도커컴포즈 모노레포는 서비스간의 네트워크가 잘 동작하는지 확인해야한다.
- 도커컴포즈 멀티레포는 각 서비스 별 도커파일과 도커컴포즈만 구성하면 된다.

### QA 및 배포 난이도가 높은 순서


도커컴포즈 멀티레포 >>> 도커컴포즈 모노레포 >>> 로컬 쿠버네티스 > 클라우드 개발서버


- 도커컴포즈 멀티레포는 개발환경에서 운영환경과 관련된 아무 설정도 하지 않았다.
- 도커컴포즈 모노레포는 K8S에 배포하기 위한 manifest와 K8S 리소스들을 설정해야한다.
- 로컬 쿠버네티스는 약간의 수정만 거치면 배포할 수 있다.
- 클라우드 개발서버는 개발이 된 거의 그대로 배포할 수 있다.

## MSA Project Example


구글에 github msa example을 검색했을 때 최상단에 나온 몇 가지 프로젝트들의 링크를 가져왔다.

### 모노레포


[https://github.com/msa-ez/example-food-delivery](https://github.com/msa-ez/example-food-delivery)

[https://github.com/msa-ez/example-library](https://github.com/msa-ez/example-library)

[https://github.com/nijin39/msa-lab/tree/main/src](https://github.com/nijin39/msa-lab/tree/main/src)

[https://github.com/msa-ez/example-mybnb](https://github.com/msa-ez/example-mybnb)

[https://github.com/msa-ez/example-academy](https://github.com/msa-ez/example-academy)

[https://github.com/ryukato/es-cqrs-axon](https://github.com/ryukato/es-cqrs-axon)

### 멀티레포


[https://github.com/AnimalHospital2](https://github.com/AnimalHospital2)

[https://github.com/msa-ez/example-rental#%EA%B5%AC%ED%98%84](https://github.com/msa-ez/example-rental#%EA%B5%AC%ED%98%84)
